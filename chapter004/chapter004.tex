\chapter{Principal Component Analysis}
\section{Maximum variance formulation}

\begin{itemize}
    \item A dataset of observations $\{X_n\}$ where $n = 1, ..., N$, and $X_n$ is a Euclidean variable with dimensionality $D$.
    \item Projecting the data onto a space having dimensionality $M<D$ \textbf{while maximizing the variance} of the projected data
\end{itemize}
 
 \begin{flushleft}
 Consider the projection onto a one-dimensional space ($M=1$).
 Defining the direction of this space using a $D-dimensional$ vector $u_1$ such that $u_1^Tu_1=1$ (unit vector)
 \end{flushleft}

\begin{equation}
    \label{eq:PCA_mean_vector}
    \overline{x} = \frac{1}{N}\sum_{n=1}^{N} x_n
\end{equation}

and the variance of the projected data is given by

\begin{equation}
    \label{eq:PCA_variance}
    \frac{1}{N}\sum_{n=1}^{N} \{u_1^Tx_n - u_1^T \overline{x}\}^2=u_1^TSu_1
\end{equation}

where $S$ is the data covariance matrix defined by

\begin{equation}
    \label{eq:PCA_covariance_matrix}
    \frac{1}{N}\sum_{n=1}^{N} \{u_1^Tx_n - u_1^T \overline{x}\}^2=u_1^TSu_1
\end{equation}

\subsubsection{Proof}
\begin{equation}
    \label{eq:PCA_covariance_proof}
    \frac{1}{N}\sum_{n=1}^{N} \{u_1^Tx_n - u_1^T \overline{x}\}^2=\frac{1}{N}\sum_{n=1}^{N}\{u_1^T(x_n - \overline{x})\}^2=\frac{1}{N}\sum_{n=1}^{N}\{u_1^T(x_n - \overline{x})u_1^T(x_n - \overline{x})\}
\end{equation}
\begin{equation}
    \label{eq:PCA_covariance_proof}
    \frac{1}{N}\sum_{n=1}^{N}\{u_1^T(x_n - \overline{x})u_1^T(x_n - \overline{x})\}=\frac{1}{N}\sum_{n=1}^{N}\{u_1^T(x_n - \overline{x})(x_n - \overline{x})^Tu_1\}=u_1^T\frac{1}{N}\sum_{n=1}^{N}\{(x_n - \overline{x})(x_n - \overline{x})^T\}u_1
\end{equation}

We now maximize the projected variance $u_1^TSu_1$ with respect to $u_1$. Clearly, this has to be a constrained maximization to prevent $||u_1|| \rightarrow \infty$. The appropriate constraint comes from the normalization condition $u_1^Tu_1=1$. To enforce this constraint, we introduce a Lagrange multiplier that we shall denote by $\lambda_1$, and then make an unconstrained maximization of

\begin{equation}
    \label{eq:PCA_optimize}
    L = u_1^TSu_1 + \lambda_1(1 - u_1^Tu_1)
\end{equation}

where $f(x)=u_1^TSu_1$ and $g(x)=1 - u_1^Tu_1$ and the equation \ref{eq: Lagrangian function}.

By setting the derivative with respect to $u_1$ equal to zero, we see that this quantity will have a stationary point when

\begin{equation}
    \label{eq:PCA_stationary_point}
    Su_1 = \lambda_1u_1
\end{equation}

which says that $u_1$ must be an eigenvector of $S$.

\subsubsection{Proof}
\begin{equation}
    \frac{\partial L}{\partial u_1}=u_1^TS - \lambda_1u_1^T
\end{equation}

$$\frac{\partial L}{\partial u_1} = 0 \Leftrightarrow (u_1^TS - \lambda_1u_1^T)=0 \Leftrightarrow u_1^TS = \lambda_1u_1^T \Leftrightarrow Su_1=\lambda u_1$$

If we left-multiply by $u_1^T$ and make use of $u_1^Tu_1=1$, we see that the variance is given by $$u_1^TSu_1=\lambda_1$$. Because $$Su_1=\lambda u_1 \Leftrightarrow u_1^TSu_1=u_1^T\lambda u_1 \Leftrightarrow u_1^TSu_1=\lambda_1 u_1^T u_1 \Leftrightarrow u_1^TSu_1=\lambda_1$$
Notice that the $\lambda_1$ is a constant number.

The variance will be maximum when we set $u_1$ equal to the eigenvector having the largest eigenvalue $\lambda_1$. This eigenvector is known as the first principal component.

